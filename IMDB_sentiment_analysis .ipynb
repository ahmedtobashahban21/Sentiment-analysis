{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75418029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.6.0-cp310-cp310-win_amd64.whl (12.3 MB)\n",
      "     ---------------------------------------- 12.3/12.3 MB 2.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in f:\\anaconda\\lib\\site-packages (from spacy) (4.64.1)\n",
      "Collecting pathy>=0.10.0\n",
      "  Downloading pathy-0.10.2-py3-none-any.whl (48 kB)\n",
      "     ---------------------------------------- 48.9/48.9 kB 1.2 MB/s eta 0:00:00\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in f:\\anaconda\\lib\\site-packages (from spacy) (2.28.1)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.7-cp310-cp310-win_amd64.whl (29 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.9-cp310-cp310-win_amd64.whl (18 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1\n",
      "  Downloading wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.4-py3-none-any.whl (11 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.8-cp310-cp310-win_amd64.whl (94 kB)\n",
      "     ---------------------------------------- 94.7/94.7 kB 1.1 MB/s eta 0:00:00\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4\n",
      "  Downloading pydantic-1.10.11-cp310-cp310-win_amd64.whl (2.1 MB)\n",
      "     ---------------------------------------- 2.1/2.1 MB 1.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: jinja2 in f:\\anaconda\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Collecting thinc<8.2.0,>=8.1.8\n",
      "  Downloading thinc-8.1.10-cp310-cp310-win_amd64.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 2.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.15.0 in f:\\anaconda\\lib\\site-packages (from spacy) (1.23.5)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in f:\\anaconda\\lib\\site-packages (from spacy) (5.2.1)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Using cached catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
      "Collecting typer<0.10.0,>=0.3.0\n",
      "  Downloading typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "     -------------------------------------- 45.9/45.9 kB 757.9 kB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=20.0 in f:\\anaconda\\lib\\site-packages (from spacy) (22.0)\n",
      "Requirement already satisfied: setuptools in f:\\anaconda\\lib\\site-packages (from spacy) (65.6.3)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Using cached langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.6-cp310-cp310-win_amd64.whl (480 kB)\n",
      "     -------------------------------------- 480.9/480.9 kB 1.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in f:\\anaconda\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in f:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.5.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in f:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in f:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in f:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.9-cp310-cp310-win_amd64.whl (7.0 MB)\n",
      "     ---------------------------------------- 7.0/7.0 MB 1.7 MB/s eta 0:00:00\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.1.0-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: colorama in f:\\anaconda\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in f:\\anaconda\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in f:\\anaconda\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n",
      "Installing collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, pydantic, murmurhash, langcodes, catalogue, blis, typer, srsly, preshed, pathy, confection, thinc, spacy\n",
      "Successfully installed blis-0.7.9 catalogue-2.0.8 confection-0.1.0 cymem-2.0.7 langcodes-3.3.0 murmurhash-1.0.9 pathy-0.10.2 preshed-3.0.8 pydantic-1.10.11 spacy-3.6.0 spacy-legacy-3.0.12 spacy-loggers-1.0.4 srsly-2.4.6 thinc-8.1.10 typer-0.9.0 wasabi-1.1.2\n"
     ]
    }
   ],
   "source": [
    "# IMDB movie review with 50k\n",
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9dd4c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wordcloud\n",
      "  Downloading wordcloud-1.9.2-cp310-cp310-win_amd64.whl (152 kB)\n",
      "     ------------------------------------ 152.1/152.1 kB 824.7 kB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.6.1 in f:\\anaconda\\lib\\site-packages (from wordcloud) (1.23.5)\n",
      "Requirement already satisfied: pillow in f:\\anaconda\\lib\\site-packages (from wordcloud) (9.4.0)\n",
      "Requirement already satisfied: matplotlib in f:\\anaconda\\lib\\site-packages (from wordcloud) (3.7.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in f:\\anaconda\\lib\\site-packages (from matplotlib->wordcloud) (1.4.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in f:\\anaconda\\lib\\site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in f:\\anaconda\\lib\\site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in f:\\anaconda\\lib\\site-packages (from matplotlib->wordcloud) (1.0.5)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in f:\\anaconda\\lib\\site-packages (from matplotlib->wordcloud) (4.25.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in f:\\anaconda\\lib\\site-packages (from matplotlib->wordcloud) (3.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in f:\\anaconda\\lib\\site-packages (from matplotlib->wordcloud) (22.0)\n",
      "Requirement already satisfied: six>=1.5 in f:\\anaconda\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "Installing collected packages: wordcloud\n",
      "Successfully installed wordcloud-1.9.2\n"
     ]
    }
   ],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bbf5282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Using cached textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n",
      "Requirement already satisfied: nltk>=3.1 in f:\\anaconda\\lib\\site-packages (from textblob) (3.7)\n",
      "Requirement already satisfied: click in f:\\anaconda\\lib\\site-packages (from nltk>=3.1->textblob) (8.0.4)\n",
      "Requirement already satisfied: tqdm in f:\\anaconda\\lib\\site-packages (from nltk>=3.1->textblob) (4.64.1)\n",
      "Requirement already satisfied: joblib in f:\\anaconda\\lib\\site-packages (from nltk>=3.1->textblob) (1.1.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in f:\\anaconda\\lib\\site-packages (from nltk>=3.1->textblob) (2022.7.9)\n",
      "Requirement already satisfied: colorama in f:\\anaconda\\lib\\site-packages (from click->nltk>=3.1->textblob) (0.4.6)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.17.1\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "459a58bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import sklearn \n",
    "import nltk \n",
    "import spacy\n",
    "import re \n",
    "import string\n",
    "import unicodedata\n",
    "import os \n",
    "import warnings\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from wordcloud import WordCloud , STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize , sent_tokenize \n",
    "from bs4 import BeautifulSoup \n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.stem import LancasterStemmer \n",
    "from sklearn.linear_model import LogisticRegression , SGDClassifier \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from textblob import TextBlob \n",
    "from textblob import Word \n",
    "from sklearn.metrics import classification_report , confusion_matrix ,accuracy_score \n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c9ca1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### read data \n",
    "path  = 'E:\\projects\\sentiment analysis\\IMDB_data\\IMDB Dataset.csv'\n",
    "data  = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95f93666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49990</th>\n",
       "      <td>Lame, lame, lame!!! A 90-minute cringe-fest th...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49991</th>\n",
       "      <td>Les Visiteurs, the first movie about the medie...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49992</th>\n",
       "      <td>John Garfield plays a Marine who is blinded by...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49993</th>\n",
       "      <td>Robert Colomb has two full-time jobs. He's kno...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49994</th>\n",
       "      <td>This is your typical junk comedy.&lt;br /&gt;&lt;br /&gt;T...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "49990  Lame, lame, lame!!! A 90-minute cringe-fest th...  negative\n",
       "49991  Les Visiteurs, the first movie about the medie...  negative\n",
       "49992  John Garfield plays a Marine who is blinded by...  positive\n",
       "49993  Robert Colomb has two full-time jobs. He's kno...  negative\n",
       "49994  This is your typical junk comedy.<br /><br />T...  negative\n",
       "49995  I thought this movie did a down right good job...  positive\n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
       "49997  I am a Catholic taught in parochial elementary...  negative\n",
       "49998  I'm going to have to disagree with the previou...  negative\n",
       "49999  No one expects the Star Trek movies to be high...  negative"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7e8a435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50000</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>49582</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Loved today's show!!! It was a variety and not...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>5</td>\n",
       "      <td>25000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   review sentiment\n",
       "count                                               50000     50000\n",
       "unique                                              49582         2\n",
       "top     Loved today's show!!! It was a variety and not...  positive\n",
       "freq                                                    5     25000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e409ac83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    25000\n",
       "negative    25000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## data is balanced \n",
    "data.sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "387558db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now time for preprocessing \n",
    "\n",
    "#let's do some steps \n",
    "\n",
    "#1. remove HTML \n",
    "#2. remove squer prackets \n",
    "#3. remove special characters \n",
    "#4. remove stopwords \n",
    "#5. stemming \n",
    "\n",
    "# finally collect all functions in one preprocessing function \n",
    "\n",
    "\n",
    "def remove_html(text):\n",
    "    soup = BeautifulSoup(text , 'html.parser')\n",
    "    return soup.get_text()\n",
    "\n",
    "def remove_squer_prackets(text):\n",
    "    return re.sub('\\[[^]]*\\]','',text)\n",
    "\n",
    "def remove_special_char(text):\n",
    "    return re.sub('[^a-zA-Z0-9\\s]','' , text)\n",
    "\n",
    "def stemming(text):\n",
    "    stem = nltk.porter.PorterStemmer()\n",
    "    text = ' '.join([stem.stem(word) for word in text.split()])\n",
    "    return text \n",
    "\n",
    "def remove_stopwords(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    filtering = [word for word in tokens if word.lower() not in stopwords]\n",
    "    return ' '.join(filtering)\n",
    "\n",
    "# collecte\n",
    "def preprocessing(text):\n",
    "    docs = remove_html(text)\n",
    "    docs = remove_squer_prackets(docs)\n",
    "    docs = remove_special_char(docs)\n",
    "    docs = stemming(docs)\n",
    "    docs = remove_stopwords(docs)\n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2772c072",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ToktokTokenizer()\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "\n",
    "# call preprocessing function \n",
    "\n",
    "processed_data =  data['review'].apply(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "074faffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's split data into train and test data \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "label_encoder = LabelBinarizer()\n",
    "labeled = label_encoder.fit_transform(data['sentiment'])\n",
    "\n",
    "\n",
    "train_data, val_data , train_label , val_label =train_test_split(processed_data ,labeled, test_size=0.1 , shuffle=True )\n",
    "\n",
    "\n",
    "train_label = pd.DataFrame(train_label)\n",
    "val_label = pd.DataFrame(val_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1aa2e886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we have two methode to train models \n",
    "\n",
    "# first ---> CountVectorizer   which mean Bag Of words  \n",
    "# second --> TfidfVectorizer   which mean Teaem frequency inverse document frequency \n",
    "\n",
    "\n",
    "# let's try each of them \n",
    "\n",
    "countV = CountVectorizer(ngram_range = (1,3) ,binary = False , min_df = 0 , max_df=1 )\n",
    "\n",
    "count_train = countV.fit_transform(train_data)\n",
    "count_val   = countV.transform(val_data)\n",
    "\n",
    "\n",
    "######################################\n",
    "\n",
    "\n",
    "TFIDF_model = TfidfVectorizer(ngram_range=(1,3) , use_idf=True , min_df = 0 , max_df=1)\n",
    "\n",
    "tf_train = TFIDF_model.fit_transform(train_data) \n",
    "tf_val   = TFIDF_model.transform(val_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dbabb495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's build models to trian it in our data \n",
    "# let's show what we have \n",
    "\n",
    "# 1.Logistic Regression model \n",
    "# 2.Stochastic Gradient Descent classify model \n",
    "# 3.naive bayes model \n",
    "\n",
    "\n",
    "# init our model with default hyperparameters \n",
    "LR_model = LogisticRegression()\n",
    "SGD_model= SGDClassifier()\n",
    "NB_model = MultinomialNB()\n",
    "\n",
    "# let's start with lgistic Regression \n",
    "\n",
    "LR_BOW  = LR_model.fit(count_train ,train_label)\n",
    "LR_TFIDF= LR_model.fit(tf_train , train_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b124f561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy from Logistic Regression model :\n",
      "BOW:\t0.7534\n",
      "TFIDF:\t0.7546\n"
     ]
    }
   ],
   "source": [
    "# now let's evaluate our model and test it \n",
    "\n",
    "\n",
    "BOW_prediction= LR_model.predict(count_val)\n",
    "tf_prediction = LR_model.predict(tf_val)\n",
    "\n",
    "BOW_accuracy= accuracy_score(BOW_prediction , val_label)\n",
    "tf_accuracy = accuracy_score(tf_prediction , val_label)\n",
    "\n",
    "\n",
    "print(f'accuracy from Logistic Regression model :\\nBOW:\\t{BOW_accuracy}\\nTFIDF:\\t{tf_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "863d9c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NB_BOW  = NB_model.fit(count_train ,train_label)\n",
    "NB_TFIDF= NB_model.fit(tf_train , train_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a7ee24ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy from Naive Bayes model :\n",
      "BOW:\t0.754\n",
      "TFIDF:\t0.7534\n"
     ]
    }
   ],
   "source": [
    "# now let's evaluate our model and test it \n",
    "\n",
    "\n",
    "BOW_prediction= NB_model.predict(count_val)\n",
    "tf_prediction = NB_model.predict(tf_val)\n",
    "\n",
    "BOW_accuracy= accuracy_score(BOW_prediction , val_label)\n",
    "tf_accuracy = accuracy_score(tf_prediction , val_label)\n",
    "\n",
    "\n",
    "print(f'accuracy from Naive Bayes model :\\nBOW:\\t{BOW_accuracy}\\nTFIDF:\\t{tf_accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "744a721d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.75      0.76      0.76      2518\n",
      "    negative       0.75      0.75      0.75      2482\n",
      "\n",
      "    accuracy                           0.75      5000\n",
      "   macro avg       0.75      0.75      0.75      5000\n",
      "weighted avg       0.75      0.75      0.75      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# as we say that naive bayes and logistic regression have the same accuracy \n",
    "# but naive bayes faster than logistic regression \n",
    "\n",
    "#let's show classification report \n",
    "\n",
    "\n",
    "clas_repo  = classification_report(BOW_prediction , val_label , target_names =['positive' , 'negative'])\n",
    "\n",
    "print(clas_repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c2e6bb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's doing some modifications in naive's hyperparamters and show result \n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'alpha': [0.1, 0.5, 1.0],\n",
    "    'fit_prior': [True, False],\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(MultinomialNB(), param_grid, cv=5)\n",
    "grid_search.fit(count_train, train_label)\n",
    "grid_search.fit(tf_train, train_label)\n",
    "# Get the best hyperparameters and model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6f86a729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy from Naive Bayes model :\n",
      "BOW:\t0.7586\n",
      "TFIDF:\t0.7586\n"
     ]
    }
   ],
   "source": [
    "# let's show accuracy \n",
    "\n",
    "\n",
    "BOW_prediction = grid_search.predict(count_val)\n",
    "TF_prediction  = grid_search.predict(tf_val)\n",
    "\n",
    "BOW_accuracy = accuracy_score(BOW_prediction , val_label)\n",
    "\n",
    "tf_accuracy = accuracy_score(TF_prediction ,val_label)\n",
    "print(f'accuracy from Naive Bayes model :\\nBOW:\\t{BOW_accuracy}\\nTFIDF:\\t{tf_accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
