{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b851bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import sklearn \n",
    "import nltk \n",
    "import re \n",
    "import string\n",
    "import unicodedata\n",
    "import os \n",
    "import warnings\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize , sent_tokenize \n",
    "from bs4 import BeautifulSoup \n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.stem import LancasterStemmer \n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "### read data \n",
    "path  = 'E:\\projects\\sentiment analysis\\IMDB_data\\IMDB Dataset.csv'\n",
    "data  = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b2c87c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbf3eac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### prepare preprocessing functions \n",
    "\n",
    "#now time for preprocessing \n",
    "\n",
    "#let's do some steps \n",
    "\n",
    "#1. remove HTML \n",
    "#2. remove squer prackets \n",
    "#3. remove special characters \n",
    "#4. remove stopwords \n",
    "#5. stemming \n",
    "\n",
    "# finally collect all functions in one preprocessing function \n",
    "\n",
    "\n",
    "def remove_html(text):\n",
    "    soup = BeautifulSoup(text , 'html.parser')\n",
    "    return soup.get_text()\n",
    "\n",
    "def remove_squer_prackets(text):\n",
    "    return re.sub('\\[[^]]*\\]','',text)\n",
    "\n",
    "def remove_special_char(text):\n",
    "    return re.sub('[^a-zA-Z0-9\\s]','' , text)\n",
    "\n",
    "def stemming(text):\n",
    "    stem = nltk.porter.PorterStemmer()\n",
    "    text = ' '.join([stem.stem(word) for word in text.split()])\n",
    "    return text \n",
    "\n",
    "def remove_stopwords(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    filtering = [word for word in tokens if word.lower() not in stopwords]\n",
    "    return ' '.join(filtering)\n",
    "\n",
    "# collect\n",
    "def preprocessing(text):\n",
    "    docs = remove_html(text)\n",
    "    docs = remove_squer_prackets(docs)\n",
    "    docs = remove_special_char(docs)\n",
    "    docs = stemming(docs)\n",
    "    docs = remove_stopwords(docs)\n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32bfe399",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ToktokTokenizer()\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "\n",
    "processed_data =  data['review'].apply(preprocessing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "636f8a6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    25000\n",
       "negative    25000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2fc88dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n",
      "Loading BERT Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#load Tokenizer and Model \n",
    "from transformers import AutoTokenizer , AutoModelForSequenceClassification\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "print('Loading BERT Model...')\n",
    "Model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", \n",
    "    num_labels = 2,   \n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False,\n",
    ")\n",
    "                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41b2cd0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (587 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length: 2072\n"
     ]
    }
   ],
   "source": [
    "#get max length  of sentences \n",
    "\n",
    "max_len = 0 \n",
    "for sent in processed_data:\n",
    "    tok_sent = tokenizer.encode(sent , add_special_tokens =True)\n",
    "    get_max = max(max_len , len(tok_sent) )\n",
    "    max_len = get_max \n",
    "    \n",
    "print(f'max length: {max_len}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5452c019",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now let's do our preprocessing to get it into model \n",
    "# we need to \n",
    "# 1.input_ids \n",
    "# 2.attention_mask \n",
    "# 3.Labels -----> it's the result\n",
    "import torch \n",
    "from torch.utils.data import Dataset , DataLoader\n",
    "\n",
    "data['sentiment'] = data['sentiment'].replace(['positive','negative'], [1,0])\n",
    "\n",
    "labels=data['sentiment'].tolist()\n",
    "input_ids = []\n",
    "attention_mask=[]\n",
    "\n",
    "\n",
    "for sent in processed_data:\n",
    "    token_text = tokenizer.encode_plus(sent , max_length=512,\n",
    "                                       add_special_tokens =True ,\n",
    "                                       pad_to_max_length=True , \n",
    "                                       return_tensors = 'pt',\n",
    "                                       return_attention_mask=True)\n",
    "    input_ids.append(token_text['input_ids'])\n",
    "    attention_mask.append(token_text['attention_mask'])\n",
    "\n",
    "input_ids = torch.cat(input_ids ,dim=0)\n",
    "attention_mask= torch.cat(attention_mask ,dim=0)\n",
    "labels= torch.tensor(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85d2e2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset , random_split\n",
    "\n",
    "\n",
    "dataset =  TensorDataset(input_ids , attention_mask ,labels)\n",
    "\n",
    "train_size = int(0.9*len(dataset)) \n",
    "val_size   = len(dataset) - train_size \n",
    "\n",
    "train_data ,val_data = random_split(dataset , [train_size , val_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "894e7910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45000, 5000)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data) , len(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08d93e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloader for train and val data \n",
    "\n",
    "from torch.utils.data import DataLoader , RandomSampler ,SequentialSampler \n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_data , sampler = RandomSampler(train_data) , batch_size=batch_size)\n",
    "\n",
    "\n",
    "val_loader   = DataLoader(val_data , sampler = SequentialSampler(val_data) , batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba63b6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## preparing for train \n",
    "\n",
    "\n",
    "from transformers import AdamW , get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "epochs = 4 \n",
    "optimizer= AdamW(Model.parameters() ,lr=2e-5 , eps=1e-8 )\n",
    "total_steps = epochs * len(train_loader)\n",
    "\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer , \n",
    "                                            num_warmup_steps=0,\n",
    "                                           num_training_steps=total_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae3d713a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are no GPU is available, just cpu\n"
     ]
    }
   ],
   "source": [
    "# chck if cdua is available\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    device =torch.device('cuda')\n",
    "    print(f'there are {torch.cuda.device_count()} GPU available')\n",
    "    print(f'we will use{torch.cuda.get_device_name(0)}')\n",
    "                         \n",
    "else:\n",
    "    print('there are no GPU is available, just cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c372b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions \n",
    "\n",
    "# first to calculate accuracy in validation loop \n",
    "\n",
    "def flat_accuracy(preds ,labels):\n",
    "    preds_arg = np.argmax(preds , axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return sum(preds_arg == labels_flat) / len(labels_flat)\n",
    "    \n",
    "\n",
    "\n",
    "# second to calculate time responding \n",
    "import time \n",
    "import datetime\n",
    "\n",
    "\n",
    "def formate_time(giv_time):\n",
    "    round_time = int(round(giv_time))\n",
    "    return str(datetime.timedelta(seconds=round_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5cf60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========epoch 1/1===========\n",
      "trianing loop....\n"
     ]
    }
   ],
   "source": [
    "# let's  train the model \n",
    "import numpy as np \n",
    "import random \n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "epochs = 1\n",
    "seed = 42 \n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "Model.to(device)\n",
    "\n",
    "training_state= []\n",
    "total_time = time.time()\n",
    "\n",
    "total_t0 =time.time()\n",
    "for epoch in range(epochs):\n",
    "    print(f'========epoch {epoch+1}/{epochs}===========')\n",
    "    print(f'trianing loop....')\n",
    "    \n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0 \n",
    "    Model.train()\n",
    "    for step, train in enumerate(train_loader):\n",
    "        \n",
    "        if step%40 ==0 and not step ==0:\n",
    "            timing = format_time(time.time()- t0)\n",
    "            print(f'batch:\\t{step} of:\\t{len(train_loader)}\\t:{timing}')\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        input_ids = train[0].to(device)\n",
    "        attention_mask= train[1].to(device)\n",
    "        labels= train[2].to(device)\n",
    "        \n",
    "        Model.zero_grad()\n",
    "        output = Model(input_ids,\n",
    "                      attention_mask=attention_mask ,\n",
    "                      labels=labels ,\n",
    "                      return_dict=True)\n",
    "        loss   = output.loss\n",
    "        logits = output.logits\n",
    "        total_train_loss += loss.item()\n",
    "        loss.back()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm(Model.parameters() , 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    average_train_loss = total_train_loss / len(train_loader)\n",
    "    total_train_time   = format_time(time.time()-t0)\n",
    "    \n",
    "    print(f'\\n')\n",
    "    print(f'average train loss: {average_train_loss}\\t total train time: {total_train_time}')\n",
    "    \n",
    "    ###############################\n",
    "    # validation loop \n",
    "    ###############################\n",
    "    \n",
    "    print(f'\\nvalidation loop....')\n",
    "    \n",
    "    total_val_accuracy=0\n",
    "    total_val_loss=0\n",
    "    t0=time.time()\n",
    "    Model.eval()\n",
    "    \n",
    "    for val in val_loader:\n",
    "        val_ids  = val[0].to(device)\n",
    "        val_mask = val[1].to(device)\n",
    "        val_label= val[2].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_out = Model(val_ids ,\n",
    "                           attention_mask=val_mask,\n",
    "                           labels = val_labels ,\n",
    "                           return_dict=True)\n",
    "        val_loss  = val_out.loss  \n",
    "        val_logits= val_out.logits\n",
    "        \n",
    "        total_val_loss+= loss.item()\n",
    "        val_logits = val_logits.detach().cpu().numpy()\n",
    "        val_labels = val_label.to('cpu').numpy()\n",
    "        \n",
    "        total_val_accuracy += flat_accuracy(val_logits , val_labels)\n",
    "        \n",
    "    average_val_accuracy = total_val_accuracy / len(val_loader)\n",
    "    total_val_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(f'average validation accuracy:{average_val_accuracy}\\t total validation time:{total_val_time}')\n",
    "    \n",
    "    training_state.append({'train loss': average_train_loss ,\n",
    "                          'train time': total_train_time ,\n",
    "                          'validation accuracy':total_val_accuracy ,\n",
    "                          'validation time':total_val_time})\n",
    "\n",
    "    \n",
    "print(\"\\n\")\n",
    "print(f'trianing complet!')\n",
    "print(f'total training took: { foramt_time(time.time()- total_t0)} . hh/mm/ss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd03e8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f926363",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
